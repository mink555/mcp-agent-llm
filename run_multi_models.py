#!/usr/bin/env python3
"""
ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ ëª¨ë¸ì— ëŒ€í•´ BFCL ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import os
import time
import argparse
from datetime import datetime
from main import run_benchmark, DEFAULT_CONFIG, BFCL_ALL_CATEGORIES

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡ (ê¸°ë³¸ê°’)
DEFAULT_MODELS = [
    # "meta-llama/llama-3.3-70b-instruct",  # Llama ì œì™¸
    "mistralai/mistral-small-3.2-24b-instruct",
    "qwen/qwen3-32b",
    "qwen/qwen3-14b",  # ì£¼ì˜: "Paid model training" ì •ì±… í•„ìš”. Privacy ì„¤ì • í™•ì¸: https://openrouter.ai/settings/privacy
    "qwen/qwen3-next-80b-a3b-instruct",
]

def run_multi_model_benchmark(samples_per_cat=10, rate_limit_delay=0, models=None):
    """ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    if models is None:
        models = DEFAULT_MODELS
    
    print("=" * 80)
    print("ğŸš€ ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
    print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(BFCL_ALL_CATEGORIES)}ê°œ (20ê°œ)")
    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë‹¹ ìƒ˜í”Œ: {samples_per_cat}ê°œ")
    print(f"â±ï¸  Rate limit delay: {rate_limit_delay}ì´ˆ")
    print(f"ğŸ¯ ì´ ì˜ˆìƒ í…ŒìŠ¤íŠ¸: {len(models)} Ã— {len(BFCL_ALL_CATEGORIES)} Ã— {samples_per_cat} = {len(models) * len(BFCL_ALL_CATEGORIES) * samples_per_cat}ê°œ")
    print("=" * 80)
    print("\ní…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡:")
    for i, model in enumerate(models, 1):
        print(f"  {i}. {model}")
    print("\n" + "=" * 80)
    
    start_time = time.time()
    results_summary = []
    
    for model_idx, model_name in enumerate(models, 1):
        print(f"\n{'=' * 80}")
        print(f"[{model_idx}/{len(models)}] ëª¨ë¸: {model_name}")
        print(f"{'=' * 80}")
        
        model_start = time.time()
        
        # ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
        config = {
            **DEFAULT_CONFIG,
            "model_name": model_name,
            "categories": list(BFCL_ALL_CATEGORIES.keys()),  # ì „ì²´ 20ê°œ ì¹´í…Œê³ ë¦¬
            "samples_per_cat": samples_per_cat,
            "rate_limit_delay": rate_limit_delay
        }
        
        try:
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            report_path = run_benchmark(config)
            
            model_elapsed = time.time() - model_start
            results_summary.append({
                "model": model_name,
                "status": "âœ… ì™„ë£Œ",
                "time": model_elapsed,
                "report": report_path
            })
            
            print(f"\nâœ… {model_name} ì™„ë£Œ (ì†Œìš” ì‹œê°„: {model_elapsed:.1f}ì´ˆ)")
            
            # ë‹¤ìŒ ëª¨ë¸ ì‹¤í–‰ ì „ ëŒ€ê¸° (ë§ˆì§€ë§‰ ëª¨ë¸ì€ ì œì™¸)
            if model_idx < len(models):
                wait_time = 10
                print(f"â³ ë‹¤ìŒ ëª¨ë¸ ì‹¤í–‰ ì „ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
                
        except Exception as e:
            model_elapsed = time.time() - model_start
            error_msg = str(e)[:100]
            results_summary.append({
                "model": model_name,
                "status": f"âŒ ì‹¤íŒ¨: {error_msg}",
                "time": model_elapsed,
                "report": None
            })
            print(f"\nâŒ {model_name} ì‹¤íŒ¨: {error_msg}")
            print("ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    total_elapsed = time.time() - start_time
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("=" * 80)
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_elapsed / 60:.1f}ë¶„ ({total_elapsed:.1f}ì´ˆ)")
    print("\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print("-" * 80)
    
    for idx, result in enumerate(results_summary, 1):
        print(f"\n{idx}. {result['model']}")
        print(f"   ìƒíƒœ: {result['status']}")
        print(f"   ì‹œê°„: {result['time']:.1f}ì´ˆ")
        if result['report']:
            print(f"   ë¦¬í¬íŠ¸: {result['report']}")
    
    print("\n" + "=" * 80)
    
    # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
    success_count = sum(1 for r in results_summary if "ì™„ë£Œ" in r['status'])
    fail_count = len(results_summary) - success_count
    
    print(f"âœ… ì„±ê³µ: {success_count}/{len(models)}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {fail_count}/{len(models)}ê°œ")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: results/")
    print("=" * 80)

def main():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹± ë° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(
        description="BFCL ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ - ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ë²¤ì¹˜ë§ˆí¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰ (20ê°œ ì¹´í…Œê³ ë¦¬ Ã— 10ê°œ ìƒ˜í”Œ Ã— 4ê°œ ëª¨ë¸)
  python run_multi_models.py
  
  # ìƒ˜í”Œ 5ê°œì”©, delay 1ì´ˆë¡œ ì‹¤í–‰
  python run_multi_models.py --samples 5 --delay 1
  
  # íŠ¹ì • ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
  python run_multi_models.py --models "openai/gpt-4o-mini" "anthropic/claude-3-haiku"
  
  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê° ì¹´í…Œê³ ë¦¬ 3ê°œì”©)
  python run_multi_models.py --samples 3 --delay 1
        """
    )
    
    parser.add_argument(
        "--samples",
        type=int,
        default=10,
        help="ì¹´í…Œê³ ë¦¬ë‹¹ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )
    
    parser.add_argument(
        "--delay",
        type=int,
        default=0,
        help="API rate limit ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 0)"
    )
    
    parser.add_argument(
        "--models",
        nargs="+",
        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: 4ê°œ ëª¨ë¸)"
    )
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    run_multi_model_benchmark(
        samples_per_cat=args.samples,
        rate_limit_delay=args.delay,
        models=args.models
    )

if __name__ == "__main__":
    main()
