#!/usr/bin/env python3
"""
ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

20ê°œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ìƒ˜í”Œ 10ê°œì”© 5ê°œ ëª¨ë¸ì„ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import os
import time
from datetime import datetime
from main import run_benchmark, DEFAULT_CONFIG, BFCL_ALL_CATEGORIES

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡
MODELS_TO_TEST = [
    "meta-llama/llama-3.3-70b-instruct",
    "mistralai/mistral-small-3.2-24b-instruct",
    "qwen/qwen3-32b",
    "qwen/qwen3-14b",  # ì£¼ì˜: "Paid model training" ì •ì±… í•„ìš”. Privacy ì„¤ì • í™•ì¸: https://openrouter.ai/settings/privacy
    "qwen/qwen3-next-80b-a3b-instruct",
]

def run_multi_model_benchmark():
    """ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("=" * 80)
    print("ğŸš€ ë‹¤ì¤‘ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìˆ˜: {len(MODELS_TO_TEST)}ê°œ")
    print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(BFCL_ALL_CATEGORIES)}ê°œ (20ê°œ)")
    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë‹¹ ìƒ˜í”Œ: 10ê°œ")
    print(f"ğŸ¯ ì´ ì˜ˆìƒ í…ŒìŠ¤íŠ¸: {len(MODELS_TO_TEST)} Ã— {len(BFCL_ALL_CATEGORIES)} Ã— 10 = {len(MODELS_TO_TEST) * len(BFCL_ALL_CATEGORIES) * 10}ê°œ")
    print("=" * 80)
    print("\ní…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡:")
    for i, model in enumerate(MODELS_TO_TEST, 1):
        print(f"  {i}. {model}")
    print("\n" + "=" * 80)
    
    start_time = time.time()
    results_summary = []
    
    for model_idx, model_name in enumerate(MODELS_TO_TEST, 1):
        print(f"\n{'=' * 80}")
        print(f"[{model_idx}/{len(MODELS_TO_TEST)}] ëª¨ë¸: {model_name}")
        print(f"{'=' * 80}")
        
        model_start = time.time()
        
        # ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
        config = {
            **DEFAULT_CONFIG,
            "model_name": model_name,
            "categories": list(BFCL_ALL_CATEGORIES.keys()),  # ì „ì²´ 20ê°œ ì¹´í…Œê³ ë¦¬
            "samples_per_cat": 10,  # ê° ì¹´í…Œê³ ë¦¬ë‹¹ 10ê°œ ìƒ˜í”Œ
            "rate_limit_delay": 0
        }
        
        try:
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            report_path = run_benchmark(config)
            
            model_elapsed = time.time() - model_start
            results_summary.append({
                "model": model_name,
                "status": "âœ… ì™„ë£Œ",
                "time": model_elapsed,
                "report": report_path
            })
            
            print(f"\nâœ… {model_name} ì™„ë£Œ (ì†Œìš” ì‹œê°„: {model_elapsed:.1f}ì´ˆ)")
            
            # ë‹¤ìŒ ëª¨ë¸ ì‹¤í–‰ ì „ ëŒ€ê¸° (ë§ˆì§€ë§‰ ëª¨ë¸ì€ ì œì™¸)
            if model_idx < len(MODELS_TO_TEST):
                wait_time = 10
                print(f"â³ ë‹¤ìŒ ëª¨ë¸ ì‹¤í–‰ ì „ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
                
        except Exception as e:
            model_elapsed = time.time() - model_start
            error_msg = str(e)[:100]
            results_summary.append({
                "model": model_name,
                "status": f"âŒ ì‹¤íŒ¨: {error_msg}",
                "time": model_elapsed,
                "report": None
            })
            print(f"\nâŒ {model_name} ì‹¤íŒ¨: {error_msg}")
            print("ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    total_elapsed = time.time() - start_time
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("=" * 80)
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_elapsed / 60:.1f}ë¶„ ({total_elapsed:.1f}ì´ˆ)")
    print("\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print("-" * 80)
    
    for idx, result in enumerate(results_summary, 1):
        print(f"\n{idx}. {result['model']}")
        print(f"   ìƒíƒœ: {result['status']}")
        print(f"   ì‹œê°„: {result['time']:.1f}ì´ˆ")
        if result['report']:
            print(f"   ë¦¬í¬íŠ¸: {result['report']}")
    
    print("\n" + "=" * 80)
    
    # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
    success_count = sum(1 for r in results_summary if "ì™„ë£Œ" in r['status'])
    fail_count = len(results_summary) - success_count
    
    print(f"âœ… ì„±ê³µ: {success_count}/{len(MODELS_TO_TEST)}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {fail_count}/{len(MODELS_TO_TEST)}ê°œ")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: results/")
    print("=" * 80)

if __name__ == "__main__":
    run_multi_model_benchmark()
